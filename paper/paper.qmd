---
title: "Predicting the 2024 US Presidential Election: A Polling-Based Forecast"
subtitle: "Trump will get support about 46.87% by pollster and have probability of 31% lead the election over Harris"
author: 
  - Tianrui Fu
  - Yiyue Deng
  - Jianing Li
thanks: "Code and data are available at: [https://github.com/FrankFU323/U.S.-presidential-election.git](https://github.com/FrankFU323/U.S.-presidential-election.git)"
date: today
date-format: long
abstract: "This paper uses advanced statistical techniques, including generalized linear regression and Bayesian models, to forecast the 2024 U.S. Presidential Election. Our analysis of polling data predicts an average support rate of 46.87% for Donald Trump and 48.50% for Kamala Harris across multiple polling sources. Among the 29 polling organizations examined, only 9 anticipate a higher support rate for Trump over Harris; however, the Bayesian model still assigns a 31% probability of Trump leading, underscoring the competitiveness of the race. These findings emphasize the importance of credible pollsters and robust statistical methods in electoral forecasting, highlighting the volatility of voter sentiment and the value of continual methodological refinement."
format: pdf
header-includes:
   - \usepackage{amsmath}
toc: true
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(kableExtra)
library(ggplot2)
library(dplyr)
library(rstanarm)
library(modelsummary)
library(modelr)
```

\newpage
# Introduction {#sec-data}

With the 2024 U.S. Presidential Election on the horizon, the nuanced interpretation of voter sentiment is paramount for anticipating, understanding, and informing the strategies of campaign teams and political analysts about electoral outcomes. This study offers a comprehensive analysis of polling data to forecast voter support for potential President, Donald Trump, focusing on how various influences—ranging from media portrayals to strategic campaign adjustments and significant socio-political developments—mold public perception and shape electoral dynamics. By utilizing a robust dataset aggregated from diverse polling sources, this research endeavours to illuminate the predictors of voter behaviour. We delve into both the quantitative metrics of polling results and the qualitative aspects, such as the credibility of polling agencies and the specific timing of polls. It is through this dual approach that our work seeks to contribute to the academic conversation by addressing the often-overlooked subtleties that affect the interpretation and reliability of polling data.

At the heart of our inquiry is the modelling of Donald Trump’s voter support, which incorporates a broad spectrum of variables including the trustworthiness of pollsters, geographic and demographic influences, and the temporal context relative to Election Day. By applying sophisticated statistical techniques such as generalized linear regression and Bayesian analysis, our study seeks to distill clear, actionable insights from the complex interplay of these factors. This methodology will not only highlight the dynamics of voter sentiment but also clarify how each determinant, from polling methodology to regional variations, impacts public opinion.

Our analysis reveals a pattern of gradually increasing support for Trump as the election nears, accentuated by the methodological integrity of the polls. This trend underscores the critical role of employing high-quality polling data to predict electoral results with greater accuracy and to strategize more effectively. Access to such data allows campaign strategies to be not only data-driven but also precisely targeted, maximizing impact where it is most effective. This approach enables campaigns to rapidly adapt to changing voter sentiments and demographic shifts, thus enhancing their responsiveness. Thus, We believe that these insights gained from rigorous data analysis are essential for refining campaign strategies and ensuring a nuanced understanding of the electoral landscape. Moreover, these insights are essential for improving the precision of electoral forecasts, thereby providing campaigns with a clearer understanding of where to allocate resources to achieve the greatest effect and sway electoral outcomes. Precise forecasts enable campaigns to allocate resources more effectively, potentially swaying the election outcomes. Moreover, the implications of this study extend to practical applications in political strategy. By elucidating the primary influences on polling statistics, the research provides campaign strategists and political consultants with the foundations to tailor their initiatives more adeptly, aiming to sway voter opinions in pivotal states and demographics.

The paper will begin with an in-depth review of our data sources and the methodologies employed, ensuring clarity and methodological transparency. The rest of the paper will be structured as follows: [Data -@sec-data] provides a detailed examination of our findings, progressing from the initial section, which analyzes the data compilation and preprocessing methods. [Model -@sec-model] discusses our modelling approach and justification, [Results -@sec-result] shows the reulst of predictions, and [Discussion -@sec-discussion] delves into the statistical analysis and its implications, with each section building upon the last to paint a comprehensive picture of the current electoral landscape.

# Data {#sec-data}

## Overview

We conducted our analysis using the statistical programming language @citeR, employing a suite of packages designed to facilitate efficient data manipulation and modeling. The analysis framework was based on the template from @citeRohan complemented by the primary packages used which include tidyverse @tidyverse for data manipulation and visualization, dplyr @dplyr for data transformation, rstanarm @rstanarm for Bayesian inference, arrow @arrow for data integration, modelr @modelr for modeling within the tidyverse framework, modelsummary @modelsummary for generating model summaries, ggplot2 @ggplot2 for creating advanced graphics, kableExtra @kableExtra for enhancing table outputs, knitr @citeknitr for dynamic report generation and here @citehere to help read the parquet. Our analysis is based on the latest polling outcomes available from the website @fivethirtyeight2024, which includes a dataset of 52 variables and 17765 observations, covering aspects such as pollster, poll score, and more.

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: tbl-dataset-example
#| tbl-cap: The example of chosen variable datset
#| echo: false

set.seed(123)
analysis_trump <- read_parquet(here::here("data/02-analysis_data/analysis_data_trump.parquet"))
analysis_harris <- read_parquet(here::here("data/02-analysis_data/analysis_data_harris.parquet"))
example_dataset <- analysis_trump[sample(1:nrow(analysis_trump), 10), c("pollster", "pollscore", "numeric_grade", "transparency_score", "end_date", "pct")]
kable(example_dataset, col.names = c("pollster", "pollscore", "numeric_grade", "transparency_score", "end_date", "pct")) %>%
  kable_styling(full_width = F, position = "center") %>%
  row_spec(0, bold = TRUE) %>%
  row_spec(0, extra_css = "border-bottom: 2px solid black;")  
```

## Cleaning Data
We have cleaned the data set and the detailed procedure see from the [Appendix -@sec-cleandata].

## Measurement

In political polling, measurement plays a critical role by translating complex and often conflicting voter sentiments into quantifiable data that can inform electoral predictions. The fundamental challenge in measurement lies in transforming a mix of opinions—where individual voters may hold conflicting views or ambivalence about candidates—into a single, structured outcome variable that reflects electoral support accurately. This process begins with careful design of polling methodologies aimed at capturing voter preferences through survey questions that effectively reduce complex sentiments into measurable data points.

This dataset, sourced from @fivethirtyeight2024, maintains high standards in polling data collection, ensuring broad voter representation and thorough documentation of variables like pollster identity, polling dates, and trust levels. In our dataset of 803 observations focused on Donald Trump's polling results, we examine 38 key variables relevant to understanding his electoral support, including the numeric grade, transparency score, poll score, pollster, end date, and percentage of support. Each variable is measured rigorously to capture both the nuances and shifts in public opinion accurately.

The results are represented as proportions of support for each candidate per poll, adjusting over time to account for factors such as changing voter sentiment in response to candidate speeches or approaching Election Day. This transformation from raw opinions to structured data enables us to track and model trends with greater predictive reliability.

In addition to measuring the outcome variable (electoral support), we also ensure accurate measurement of predictor variables, such as pollster transparency scores and numeric grades, which may affect a poll's reliability. By applying these measurement principles, we systematically convert public opinion into a structured dataset that provides a robust basis for predictive modeling. Ultimately, this rigorous approach allows us to build a Bayesian model to evaluate how various pollsters may influence final election outcomes, thereby bridging individual opinions and broader electoral predictions with a high degree of credibility.

## Outcome variables

The pct is the percentage of the vote or support that the candidate received in the poll and keep integer. The @fig-percentage shows the distribution of this variable, labeled "pct," with most values clustered around 47%. The distribution appears approximately normal but has a slight skew due to a few higher values. Overlaying the @fig-percentage is a density curve that illustrates the general shape of the data. Accompanying the @fig-percentage is @tbl-pct, a summary statistics table for the "pct" variable, which indicates a mean of 46.98%, a median of 47%, and a standard deviation of 3.8. The range spans from 31% to 70%, showing some spread in the data, with most observations relatively close to the mean, suggesting that while there is some variability, support percentages are largely concentrated around the central values.

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: fig-percentage
#| fig-cap: Distribution of percentage
#| echo: false
ggplot(analysis_trump, aes(x = pct)) +
  geom_histogram(binwidth = 1.5, alpha = 0.7, fill = "#87CEEB", color = "black") +
  geom_density(aes(y = after_stat(count)), color = "#D55E00", alpha = 0.5) +
  labs(x = "Percentage (pct)",
       y = "Count") +
  theme_minimal()
```

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: tbl-pct
#| tbl-cap: Summary Statistics for 'pct' Variable
#| echo: false
summary_pct <- analysis_trump %>%
  summarise(
    Mean = round(mean(pct, na.rm = TRUE), 2),
    Median = round(median(pct, na.rm = TRUE), 2),
    Lower = round(min(pct, na.rm = TRUE), 2),
    Upper = round(max(pct, na.rm = TRUE), 2),
    SD = round(sd(pct, na.rm = TRUE), 2)
  )

kable(summary_pct) %>%
  kable_styling(full_width = F, position = "center", bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Predictor variables

### Pollster

The pollster is the name of the polling organization that conducted the poll included Marquette Law School, CNN/SSRS and etc. In @fig-frequency-bar, the predictor variable is the count of polls conducted by each pollster. The bar chart displays the number of polls released by different polling organizations, ordered from highest to lowest frequency. The bars range in color from light blue to dark blue, indicating the frequency of polls conducted by each organization. Siena/NYT and YouGov are the most active pollsters, with 136 and 121 polls conducted, respectively. Emerson and AtlasIntel also have high polling frequencies, with 73 and 82 polls. In contrast, several pollsters, like Christopher Newport University and McCourtney Institute/YouGov, conducted only one poll, indicating their minimal activity in comparison.

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: fig-frequency-bar
#| fig-cap: Frequency of each pollster
#| echo: false
ggplot(analysis_trump, aes(y = reorder(pollster, table(pollster)[pollster]))) +  
  geom_bar(aes(fill = after_stat(count)), color = "black") +                          
  geom_text(stat = "count", aes(label = after_stat(count)), hjust = -0.1, size = 2) +  
  scale_fill_gradient(low = "lightblue", high = "blue") +
  labs(x = "Count", y = "Pollster") +       
  theme_minimal() +
  theme(axis.text.x = element_text(size = 8)) 
```
### Poll Score

The poll score is the numeric value representing the score or reliability of the pollster in question from -1.5 to -0.5 and negative numbers of poll score are better. @fig-pollscore is a vertical bar chart displaying the distribution of poll scores, ranging from -0.5 to -1.5. The most frequent poll score is -1.1, indicating a high concentration around this value. Most pollsters' poll scores are below -0.8, which suggests that overall scores are relatively low, reflecting a higher level of trust in the polls. Further analysis shows that the lower the poll score, the higher the trust level. Therefore, the distribution is notably skewed, with most data concentrated in the lower range (around -1.1 and below). Higher negative scores (closer to -0.5) are relatively rare, indicating that most polling agencies demonstrated high credibility in this survey, earning broad approval from the public or voters.

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: fig-pollscore
#| fig-cap: Distribution of Poll Score
#| echo: false
#| message: false
#| warning: false

ggplot(analysis_trump, aes(x = pollscore)) +
  geom_histogram(binwidth = 0.1, fill = "#87CEEB", color = "black") +
  labs(x = "Poll Score",
    y = "Count"
  ) +
  theme_minimal()
```
### Transparency Score

The transparency_score is the grade for how transparent a pollster is, calculated based on how much information it discloses about its polls and weighted by recency from 4.5 to 10.0. @fig-transparency illustrates the distribution of "Transparency Scores" related to the 2024 election, possibly representing public or media ratings of candidate transparency, ranging from 5 to 10. The majority of scores are clustered around 8, showing that most respondents rate transparency fairly high. Scores of 6 and 7 are less frequent, while extreme scores of 5 and 10 are rare. This indicates that most voters or media sources perceive candidates as moderately to highly transparent, with scores skewed toward the higher end of the scale.

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: fig-transparency
#| fig-cap: Distribution of Transparency Score
#| echo: false
#| message: false
#| warning: false

ggplot(analysis_trump, aes(x = transparency_score)) +
  geom_histogram(binwidth = 1, fill = "#87CEEB", color = "black") +
  labs(
    x = "Transparency Score",
    y = "Count"
  ) +
  theme_minimal()
```
### Numeric Grade

The numeric_grade is the numeric rating given to the pollster to indicate their quality or reliability from 2.7 to 3.0. @fig-numeric displays the distribution of a "Numeric Grade" associated with the 2024 election, which could represent candidate approval ratings, public opinion scores, or voter satisfaction. The grades are primarily concentrated between 2.7 and 3.0, with a noticeable peak close to 3.0, indicating a higher frequency of favorable ratings. This distribution suggests strong public support or positive sentiment, with the majority of respondents giving relatively high scores on this variable.

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: fig-numeric
#| fig-cap: Density Plot of Numeric Grade
#| echo: false
#| message: false
#| warning: false
ggplot(analysis_trump, aes(x = numeric_grade)) +
  geom_density(fill = "#87CEEB", alpha = 0.4) +
  labs(
    x = "Numeric Grade",
    y = "Density"
  ) +
  theme_minimal()
```
### End date

The end_date is the date of polling ends. @fig-enddate shows the distribution of a particular variable over time related to the 2024 U.S. election. The x-axis represents dates from August to November, and the y-axis shows the count. This variable might represent the frequency of certain events, such as news coverage, voter registrations, or polling responses. The data is relatively sparse from August to October, with small peaks in some areas. In November, however, there is a significant increase in counts, particularly in early November, possibly linked to the heightened election activity or voter engagement as Election Day approaches.

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: fig-enddate
#| fig-cap: Frequency of Records by End Date
#| echo: false
#| message: false
#| warning: false

ggplot(analysis_trump, aes(x = end_date)) +
  geom_bar(fill = "#87CEEB", color = "black") +
  labs(
    x = "End Date",
    y = "Count"
  ) +
  theme_minimal()
```

### Relationship between pollster, pollscore and end date

In @fig-avg-pollscore, the predictor variable is the average poll score for each pollster over time, from August to October. This heat map visualizes the poll scores of each organization on different dates, with color intensity indicating the score levels—darker colors represent lower scores. The pollsters with the most consistently low average poll scores (darker colors) include Selzer, Siena/NYT, and Marquette Law School. These pollsters frequently show results lower than others over time, suggesting they may consistently lean toward one direction in their results. In contrast, pollsters like YouGov Center for Working Class Politics and YouGov Blue display lighter colors, indicating relatively higher scores across their polls.

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: fig-avg-pollscore
#| fig-cap: Average Pollscore by Pollster and End Date
#| echo: false
library(dplyr)

heatmap_data <- analysis_trump %>%
  group_by(pollster, end_date) %>%
  summarise(mean_pollscore = mean(pollscore, na.rm = TRUE), .groups = "drop") 

ggplot(heatmap_data, aes(x = end_date, y = pollster, fill = mean_pollscore)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(x = "End Date", y = "Pollster", fill = "Avg Pollscore") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(size = 8))
  
```

### Relationship between transparency score and numeric grade

In @fig-plot, there is a clear positive relationship between transparency score and numeric grade. Pollsters with higher transparency scores, such as those scoring around 10, tend to achieve higher numeric grades, close to 3.0. Conversely, pollsters with lower transparency scores tend to have lower numeric grades, closer to 2.7. This pattern suggests that greater transparency is associated with better overall pollster ratings.

```{r, fig.pos= 'H', fig.width=8, fig.height=4}
#| label: fig-plot
#| fig-cap: Scatter Plot of transparency score and numeric grade
#| echo: false
#| message: false
#| warning: false
ggplot(analysis_trump, aes(x = transparency_score, y = numeric_grade)) +
  geom_point(color = "darkblue", alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(x = "Transparency Score", y = "Numeric Grade") +
  theme_minimal()
```

# Model {#sec-model}

## Model Selection

Our model aims to predict the changes in Donald Trump’s probability of winning the 2024 U.S. election over time while examining the influence of key factors, such as pollster reliability scores, transparency, and specific pollster effects, on polling results. For model selection, we initially considered a generalized linear regression to provide a straightforward analysis of the relationships between these factors. However, given the limitations of generalized linear regression in handling uncertainty and dynamic data, we ultimately chose a Bayesian model. The Bayesian approach enables the integration of prior information and dynamically updates predictions, offering greater stability and accuracy under high uncertainty. Below is a brief overview of our model.

Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up
In this Bayesian framework, we assume a normal distribution of poll results around a mean affected by key predictors: numeric grade, transparency score, pollscore, pollster and end date. Define $y_i$ as the percentage of Donald Trump.

### Model 1 -- GLM

\begin{align} 
y_i &= \alpha + \beta_1 \cdot \text{numeric\_grade}_i + \beta_2 \cdot \text{transparency\_score}_i + \beta_3 \cdot \text{pollscore}_i \\
&\quad + \sum_{j=1}^{N} \gamma_j \cdot \text{pollster}_j + \delta \cdot \text{end\_date}_i + \epsilon_i 
\end{align}

where: 

- $\alpha$ is the intercept, representing the average poll level.

- $\beta_1$, $\beta_2$, and $\beta_3$ are the regression coefficients for numeric grade, transparency score, and pollscore, respectively.

- $\gamma_j$ indicates the fixed effect of each pollster.

- $\delta$ is the regression coefficient for the end date.

- $\epsilon_i$ is the error term.

### Model 2 -- Bayesian model for Trump

\begin{align}
y_i | \mu_i, \sigma &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 \cdot \text{numeric\_grade}_i + \beta_2 \cdot \text{transparency\_score}_i + \beta_3 \cdot \text{pollscore}_i \\
&\quad + \sum_{j=1}^{N} \gamma_j \cdot \text{pollster}_j + \delta_i \cdot \text{end\_date}_i \\
\alpha &\sim \text{Normal}(50, 10) \\
\beta_1, \beta_2, \beta_3 &\sim \text{Normal}(0, 5) \\
\gamma_j &\sim \text{Normal}(0, 5) \\
\sigma &\sim \text{Exponential}(1)
\end{align}

where:

- $\alpha$ represents an average poll result.

- $\beta_1$, $\beta_2$, and $\beta_3$ capture the unique effect of their respective predictors on the poll percentage.

- $\gamma_j$ denotes a specific pollster effect.

- $\delta_i$ accounts for time trends.

We run the model in @citeR using the package of rstanarm @rstanarm.

## Model justification

The choice of a Bayesian model to analyze Donald Trump’s voting support rate stems from the need to effectively integrate uncertainty and leverage prior knowledge in estimating outcomes. Given that the dataset includes key predictors related to polling organizations—such as numeric grade, transparency score, pollster, poll score, end date, and pct—this approach allows for a comprehensive understanding of how these variables influence voting results.

The numeric grade and transparency score are crucial in assessing the reliability of polling organizations. By integrating these factors, the Bayesian model can provide a nuanced analysis of how the quality and transparency of polling organizations impact Trump’s support rate. This is particularly important in the context of public opinion polling, as perceptions of reliability can significantly affect the interpretation of results.

Moreover, the Bayesian framework enables the incorporation of prior distributions for model parameters, reflecting beliefs about their possible values before observing the data. This is especially beneficial in a domain where historical data and expert opinion can provide valuable insights.

Additionally, by including the pollster variable, the model accounts for the fixed effects of different polling organizations, allowing for a more targeted analysis that acknowledges the unique characteristics of each organization. The end_date variable helps to consider time trends, ensuring that the analysis remains relevant to the evolving political landscape, especially as elections approach.

However, there are areas for improvement within the model. First, refining the selection of variables related to voting support could enhance the model’s predictive power. For instance, incorporating socio-economic factors, demographic data, or shifts in voter behavior may provide a more comprehensive perspective. Additionally, the model's predictive performance could be assessed through cross-validation or other model evaluation techniques to ensure the robustness and reliability of the results.

By employing this Bayesian model, our aim is to provide robust estimates of Trump’s support rate, considering both the statistical characteristics of the data and the inherent uncertainty related to polling, while exploring avenues for further model improvement.

# Results {#sec-result}

The results part is combined with the result for models of analysis data and the result of predictions by Bayesian model on November 5, 2024. In order to get to know whether the votes of Trump will win the election, we combined the data of Harris who have the most competitive candidate other than Trump to do predictions. It would help us be faster and more accurate to do the predict judgment.

## Result of model for analysis data

In @fig-final, we applied a generalized linear regression model to predict the percentage of polls Trump according each pollster. Specifically, the support rate data points range from 40% to 55%, with most clustered between 45% and 50%. The orange trend line shows a subtle upward trend, indicating a gradual increase in Trump's average support rate over time. For example, in early August, the support rate is around 45%, while by mid-October, the average support rate shown by the trend line is close to 50%. This suggests that Trump's support rate has risen by about 5% over these three months. For the model summary of Generalized Linear Regression and Bayesian Model, the table can see at [Appendix -@sec-tbl-model-summary].

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)
library(here)

model_final_glm <-
  readRDS(file = here::here("models/model_final.rds"))
model_bayes_trump <-
  readRDS(file = here::here("models/model_bayes_trump.rds"))
model_bayes_harris <-
  readRDS(file = here::here("models/model_bayes_harris.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: fig-final
#| fig-cap: Donald Trump Support Over Time by Generalized Linear Regression
#| warning: false
#| message: false
library(ggplot2)
library(modelsummary)
library(modelr)
analysis_trump <- analysis_trump |>
  add_predictions(model_final_glm, var = "final")

ggplot(analysis_trump, aes(x = end_date)) +
  geom_point(aes(y = pct), color = "#87CEEB", size = 2, alpha = 0.7) +
  geom_smooth(aes(y = final), color = "#D55E00", size = 1.2, se = FALSE) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +
  theme_classic() +
  labs(y = "Trump Percent", x = "Date") +
  theme(axis.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1))
```
In @fig-bayes-trump, we applied a bayesian model to predict the percentage of polls Trump according each pollster. Each data point is color-coded by pollster, with support rates ranging from 35% to 60%. The shaded gray area around the trend line represents the confidence interval, indicating the uncertainty in support rate variations. The overall trend line (blue) shows a slight upward trend, increasing from about 45% in August to nearly 50% in October. In particular, data points in early August are more dispersed, with some pollsters reporting low support rates (around 40%) and others reporting higher rates (over 50%). By November, most poll results have converged, with support rates centered between 45% and 50%. Notably, certain pollsters, like YouGov and Marist, provide either consistently higher or lower support rate estimates compared to others, indicating some bias or variation among polling organizations.

We also applied a Bayesian model to predict the percentage of polls Harris according each pollster. The figure can see it in [Appendix -@sec-bayes-harris].

```{r, fig.pos= 'H', fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: fig-bayes-trump
#| fig-cap: Poll Percentage over Time with Bayesian Fit for Trump
#| warning: false

new_data <- data.frame(
  end_date = seq(
    min(analysis_trump$end_date),
    max(analysis_trump$end_date),
    length.out = 100
  ),
  numeric_grade = mean(analysis_trump$numeric_grade),
  pollscore = mean(analysis_trump$pollscore),
  transparency_score = mean(analysis_trump$transparency_score),
  pollster = factor("AtlasIntel")
)

posterior_preds <- posterior_predict(model_bayes_trump, newdata = new_data)


pred_summary <- new_data |>
  mutate(
    pred_mean = colMeans(posterior_preds),
    pred_lower = apply(posterior_preds, 2, quantile, probs = 0.025),
    pred_upper = apply(posterior_preds, 2, quantile, probs = 0.975)
  )


ggplot(analysis_trump, aes(x = end_date, y = pct, color = pollster)) +
  geom_point() +
  geom_line(
    data = pred_summary,
    aes(x = end_date, y = pred_mean),
    color = "blue",
    inherit.aes = FALSE
  ) +
  geom_ribbon(
    data = pred_summary,
    aes(x = end_date, ymin = pred_lower, ymax = pred_upper),
    alpha = 0.2,
    inherit.aes = FALSE
  ) +
  labs(
    x = "End Date",
    y = "Percentage"
  ) +
  theme_minimal() +
  theme(
    legend.key.size = unit(1, "cm"),
    legend.text = element_text(size = 5) 
  )

```
## Result for model prediction

@fig-pred-mean compares the predicted mean support rates for Trump and Harris across different polling organizations in 5th November of 2024. The vertical axis shows the predicted support rate (in percentage), and the horizontal axis lists the various pollsters. The red line represents Harris's predicted support rate, while the blue line represents Trump's.

For example, in predictions from Christopher Newport U. and MassINC Polling Group, Harris has a noticeably higher average support rate than Trump, exceeding 50%, while Trump's support rate is below 50%. In Siena's predictions, Trump’s support rate is slightly lower than Harris’s, reaching around 41%, while Harris’s support is close to 53%. In Christopher Newport U.'s, MassINC Polling Group's, Siena's and Washington Post/George Mason University's predictions, Trump's support rate is all lower than Harris's and have a big gap.

```{r, fig.pos= 'H', fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: fig-pred-mean
#| fig-cap: Predicted Mean Percentage by Pollster for Trump and Harris
#| warning: false

all_pollsters_trump <- unique(analysis_trump$pollster)
all_pollsters_harris <- unique(analysis_harris$pollster)

all_predictions <- data.frame()

for (pollster in all_pollsters_trump) {
  new_data_trump <- data.frame(
    end_date = as.Date("2024-11-05"),
    numeric_grade = mean(analysis_trump$numeric_grade, na.rm = TRUE),
    pollscore = mean(analysis_trump$pollscore, na.rm = TRUE),
    transparency_score = mean(analysis_trump$transparency_score, na.rm = TRUE),
    pollster = factor(pollster) 
  )
  
  posterior_preds_trump <- posterior_predict(model_bayes_trump, newdata = new_data_trump)
  
  final_prediction_trump <- data.frame(
    pollster = pollster, 
    candidate = "Trump",
    pred_mean = mean(posterior_preds_trump),
    pred_lower = quantile(posterior_preds_trump, probs = 0.025),
    pred_upper = quantile(posterior_preds_trump, probs = 0.975)
  )
  
  all_predictions <- rbind(all_predictions, final_prediction_trump)
}

for (pollster in all_pollsters_harris) {
  new_data_harris <- data.frame(
    end_date = as.Date("2024-11-05"),
    numeric_grade = mean(analysis_harris$numeric_grade, na.rm = TRUE),
    pollscore = mean(analysis_harris$pollscore, na.rm = TRUE),
    transparency_score = mean(analysis_harris$transparency_score, na.rm = TRUE),
    pollster = factor(pollster) 
  )
  
  posterior_preds_harris <- posterior_predict(model_bayes_harris, newdata = new_data_harris)
  
  final_prediction_harris <- data.frame(
    pollster = pollster, 
    candidate = "Harris",
    pred_mean = mean(posterior_preds_harris),
    pred_lower = quantile(posterior_preds_harris, probs = 0.025),
    pred_upper = quantile(posterior_preds_harris, probs = 0.975)
  )
  
  all_predictions <- rbind(all_predictions, final_prediction_harris)
}

ggplot(all_predictions, aes(x = pollster, y = pred_mean, color = candidate, group = candidate)) +
  geom_line(linewidth = 1.2) +
  geom_point() +  
  labs(
    x = "Pollster",
    y = "Predicted Mean (%)",
    color = "Candidate"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

In @tbl-pred-wide, we merged and compared the Bayesian model predictions of the vote shares for Trump and Harris on November 5, 2024, by pollster. The green background color represents the same pollster, indicating that the side with the larger predicted vote share is expected to win based on the data collected and summarized by that pollster. Conversely, red indicates the losing side. In @tbl-pred-wide, a total of 29 pollsters participated in the polling, with 9 pollsters predicting a Trump victory and 18 pollsters predicting a Harris victory. In @tbl-pred-summary, we calculated the mean support for Trump and Harris, estimating Trump's probability of leading probability to be 31% and the mean support be 46.87%. For the further more predictions about the Bayesian model, we can see at [Appendix -@sec-pred-summary].

```{r, fig.pos= 'H', fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: tbl-pred-wide
#| tbl-cap: Predictions for both Trump and Harris by pollster
#| warning: false
library(dplyr)
library(kableExtra)

predictions_wide <- reshape(all_predictions, idvar = "pollster", timevar = "candidate", direction = "wide")

predictions_wide <- predictions_wide |>
  mutate(
    winner = ifelse(pred_mean.Trump > pred_mean.Harris, "Trump", "Harris"),
    prob_trump_lead = ifelse(pred_mean.Trump > pred_mean.Harris, 1, 0)
  )


table_output <- predictions_wide %>%
  select(pollster, pred_mean.Trump, pred_mean.Harris) %>% 
  kable(booktabs = TRUE) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"), font_size = 8) %>%
  column_spec(1, width = "1cm") %>%
  column_spec(2, bold = TRUE, width = "4cm") %>% 
  column_spec(3, width = "3.3cm", background = ifelse(predictions_wide$pred_mean.Trump > predictions_wide$pred_mean.Harris, "#CCFFCC", "#FFCCCC")) %>%  
  column_spec(4, width = "3.3cm", background = ifelse(predictions_wide$pred_mean.Harris > predictions_wide$pred_mean.Trump, "#CCFFCC", "#FFCCCC")) %>%
  add_header_above(c(" " = 1, "Predictions Summary" = 3)) %>% 
  row_spec(0, bold = TRUE)

table_output <- table_output %>%
  add_footnote("Green indicates the pollster predicts the winning side, while red indicates the losing side.")

table_output
```
```{r, fig.pos= 'H', fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: tbl-pred-summary
#| tbl-cap: Summary of Predictions by mean and lead probability
#| warning: false

trump_mean <- mean(predictions_wide$pred_mean.Trump, na.rm = TRUE)
harris_mean <- mean(predictions_wide$pred_mean.Harris, na.rm = TRUE)
trump_lead_prob <- mean(predictions_wide$prob_trump_lead, na.rm = TRUE)

summary_table <- data.frame(
  Metric = c("Trump Mean Support", "Harris Mean Support", "Trump Lead Probability"),
  Value = round(c(trump_mean, harris_mean, trump_lead_prob), 2)
)

library(kableExtra)
kable(summary_table, col.names = c("Metric", "Value")) %>%
  kable_styling(full_width = FALSE, position = "center")
```


# Discussion {#sec-discussion}

## Summary of findings

The predictive models deployed in our study delineate a subtle yet pivotal dynamic: economic indicators and significant public events exert substantial influence on voter behavior as elections approach. The Bayesian analysis reveals a close support rate between Donald Trump (46.87%) and Kamala Harris (48.50%), highlighting an electorate acutely responsive to shifts in economic sentiment and public perception. This responsiveness is intensified by the fact that nearly 41% of pollsters show varying support levels for Trump, underscoring a voter sentiment that is not merely divided but remarkably fluid. This data suggests that electoral outcomes can pivot rapidly following major economic updates or notable public engagements by candidates, potentially reshaping the electoral narrative almost instantaneously.

Furthermore, this analysis accentuates the imperative for campaign strategies to remain highly adaptable to these evolving conditions. The efficacy of Bayesian models, which we will talk about in the next section, in integrating and updating predictions with incoming economic data and changes in public sentiment is particularly invaluable. This capability enables campaigns to refine their strategic approaches almost in real-time, customizing their communications to exploit shifts in voter sentiment precipitated by economic improvements or declines. Such strategic agility is crucial; it equips campaign teams to seize opportunities to influence undecided or swing voters at decisive moments, effectively harnessing economic and social currents to secure electoral leverage.


## Evaluating Predictive Models in Electoral Forecasting {#sec-first-point}

In the task of forecasting the U.S. presidential election, the strategic selection of predictive models is paramount due to the complex interplay of electoral dynamics, such as shifts in voter behavior driven by economic changes, social movements, and regional voting patterns. These dynamics are characterized by non-linear relationships and interactions that are not only highly contextual but also vary significantly over time. Generalized Linear Models (GLMs) are valued for their straightforwardness and computational efficiency. However, they often simplify the intricate relationships within electoral data, which can result in overlooking subtle yet crucial nuances that are vital for accurate predictions. In contrast, Bayesian models excel in environments where prior knowledge and expert insights are available. These models integrate these elements through prior distributions that inform and refine the model’s estimations, making them particularly advantageous for electoral forecasting. This integration allows for a nuanced understanding of voter intentions, providing the ability to adaptively update as new data becomes available— a critical feature in the rapidly changing landscape of political campaigns.

Additionally, Bayesian models produce probabilistic outputs—posterior distributions of parameters—that offer not only point estimates but also the probabilities of these estimates. This aspect enhances the interpretability and reliability of the forecasted electoral outcomes, making Bayesian models especially adept at handling the uncertainties inherent in polling data, such as non-response biases and sampling errors. Our project’s application of Bayesian methodologies not only evaluates but also quantifies the effects of various predictive factors within a sophisticated probabilistic framework, ensuring a robust approach to capturing the complexities of national election predictions.

## Quantifying Pollster Influence on Electoral Outcomes

The influence of pollster methodology on electoral forecasts is particularly critical for candidates like Donald Trump, whose support base may be heterogeneously distributed and exhibits high polarization. Our analysis of aggregated polling data from various sources demonstrates significant variations in predictions due to differing pollster methodologies. This variability highlights the crucial need for a rigorous assessment of how pollster-specific biases and methodological differences can skew the overall election forecasts. To systematically address this issue, an in-depth evaluation of each pollster’s historical accuracy and methodological rigor is required. Pollsters with a track record of high accuracy, especially in contexts demographically and politically akin to the current electoral environment, should be weighted more heavily in the models.

Furthermore, adopting a Bayesian hierarchical approach can refine these forecasts by pooling data from a variety of sources and adjusting for known biases. This model enhances the predictions by modulating the influence of each pollster based on their historical performance and methodological transparency, thereby improving the reliability and accuracy of electoral forecasts. Such a comprehensive approach not only mitigates the impact of pollster-specific biases but also provides a more detailed and contextually informed perspective on potential electoral outcomes.

## Weaknesses and next steps

While our predictive models provide a comprehensive analysis of electoral dynamics, they are not without limitations. One notable challenge is the potential for overfitting in complex Bayesian models, which may adapt too closely to specific datasets and fail to generalize across different electoral cycles. Additionally, despite the sophisticated integration of economic indicators and polling data, the unpredictable nature of voter behavior—especially in response to sudden political or economic events—presents a significant challenge to the accuracy of our forecasts. The reliance on historical data to inform model priors can also lead to biases if past trends do not accurately reflect future outcomes.

To address these challenges, future research should focus on enhancing the generalizability of our models. This could involve the development of new statistical techniques that reduce the risk of overfitting and improve the models’ ability to adapt to new and varying datasets. Incorporating real-time data analytics into our Bayesian framework could also enhance the responsiveness of our models to sudden changes in the political landscape. Furthermore, expanding our data sources to include more granular demographic and psychographic information could refine our understanding of voter behavior and improve the accuracy of our predictions.

Additionally, conducting longitudinal studies to track changes in voter sentiment over multiple election cycles could provide deeper insights into the long-term efficacy of our predictive models and help identify persistent biases in pollster methodologies. Finally, increasing collaboration with political scientists and behavior analysts could enrich the theoretical foundations of our models, ensuring that they not only predict electoral outcomes but also contribute to a broader understanding of electoral behavior.

\newpage

\appendix

# Appendix {#sec-appendix}

## Methodology Overview and Evaluation of YouGov Polling

**Introduction**

YouGov is an online polling company using a mix of non-probability sampling and a specialized model called multilevel regression with post-stratification (MRP) to estimate voter intentions. This appendix reviews YouGov’s methodology, covering sample recruitment, data collection, non-response management, questionnaire design, and MRP modeling. An assessment of the strengths and limitations of YouGov’s approach is also provided.

**Population, Frame, and Sample Composition**

The target population for YouGov’s election polling consists of American adults, particularly registered voters. YouGov uses an online panel framework, recruiting participants who voluntarily sign up and provide detailed demographic information. The sampling frame is further refined by aligning samples with demographic data from the TargetSmart voter file, which ensures that the sample accurately reflects national voter demographics. YouGov adjusts sample sizes during election cycles to improve representativeness and model robustness, starting with approximately 100,000 responses and increasing by an additional 20,000 during peak months, such as September and October.

**Sample Recruitment and Representativeness**

Panelists are recruited online through digital ads and partnerships with other websites, allowing any American adult with internet access to participate. Demographic data provided by participants enables YouGov to weight samples to reflect the population more closely. Respondents earn points redeemable for small rewards, which promotes engagement and data quality. However, reliance on online recruitment can underrepresent groups with limited internet access, such as some rural and low-income populations.

**Sampling Approach and Methodological Trade-offs**

YouGov’s sampling is non-probabilistic, meaning not everyone has an equal chance of selection. To mitigate this, YouGov applies MRP, a hierarchical modeling technique that segments respondents into subgroups based on characteristics like age, gender, race, education, and region. Subgroups are weighted to match their proportions in the broader population. While MRP efficiently models population-wide estimates, it may have trade-offs, especially in accurately representing smaller states and hard-to-reach demographic groups.

**Non-response Handling and Quality Control Measures**

YouGov employs quality control checks to address non-response bias. The platform incorporates response speed and consistency checks to filter out low-quality responses, and panelists failing these criteria are removed from future studies. These quality measures enhance the MRP model's accuracy by ensuring reliable data, critical for generating robust estimates of voter intentions.

**Questionnaire Design**

YouGov’s questionnaires are concise and randomized to minimize bias, with neutral wording and varied question types (text, images, audio) to enhance respondent understanding. For sensitive topics, options like "prefer not to say" are included to protect privacy. The online format, however, excludes certain demographics, and shorter survey lengths may limit the depth of data collected.

**MRP Model for Vote Estimation**

YouGov’s MRP model estimates voter support through three main steps: predicting voter turnout likelihood, determining likely voter candidate support, and aggregating to estimate overall support. The model draws on responses from the TargetSmart voter file, enabling estimates at both national and regional levels. MRP also allows YouGov to track voter opinion shifts, reflecting changing public sentiment over time.

**Strengths and Limitations of the YouGov Approach**

YouGov’s methodology has strengths, including the MRP model’s ability to generate precise regional and national predictions, and voter file use that enhances sample validity. Continuous panelist engagement allows YouGov to monitor shifts in voter support. However, potential limitations stem from the non-probability sampling approach, which may affect representativeness, especially in smaller states or underrepresented groups. Additionally, reliance on internet-based methods may miss some populations lacking consistent internet access.

**Conclusion**

YouGov employs a structured methodology combining MRP modeling, quality control measures, and a large online panel, producing valuable insights into U.S. voter intentions. While challenges in achieving full representativeness and internet reach persist, YouGov’s methodology has demonstrated success in past elections and serves as a reliable tool for future polling and tracking electoral trends.

\newpage
## Idealized Survey Methodology

The budget for this survey is $100,000, aimed at predicting the outcome of the 2024 United States Presidential Election. This methodology includes stratified sampling, multi-platform recruitment, data validation, and multi-wave data aggregation to ensure representative and reliable data.

**Sampling Approach: Stratified Random Sampling**

To obtain a representative sample, we use stratified random sampling with a sample size of 5,000 respondents. Sampling is stratified by age, gender, education, and geographic region to ensure broad coverage across voter demographics. The age groups include 18-29, 30-44, 45-64, and 65 and above. Gender is categorized as male, female, and other, while education is classified as high school or below, bachelor’s degree, and master’s degree or above. The geographic region includes all U.S. states. Stratified sampling ensures that the sample accurately reflects voter characteristics, providing a solid foundation for subsequent analysis.

**Recruitment: Multi-Platform and Interactive Engagement**

Recruitment is conducted through a multi-platform strategy to ensure wide coverage among voters. Targeted ads are deployed on Google, Facebook, and Twitter to attract respondents with specific demographic characteristics. The ad content is concise and highlights anonymity and the research purpose to encourage participation. In addition, we use Random Digit Dialing (RDD) to contact older adults and residents in remote areas who may be less accessible online, ensuring their participation in the survey. To increase the completion rate, a small reward system is implemented, with one out of every 100 participants receiving a $5 to $10 incentive.

**Survey Platform: Google Forms and Phone Outreach**

Google Forms is used as the primary data collection platform, enabling respondents to complete the survey on a computer or mobile device with ease. We have set Google Forms to restrict submissions to one per Google account to prevent duplicate responses. For those who may not have easy online access, telephone outreach will be conducted, especially targeting older adults and rural voters, to ensure the diversity and inclusivity of the sample.

**Data Validation: Post-Stratification Weighting**

To ensure data quality, post-stratification weighting will be applied during the analysis phase based on demographic characteristics. This adjustment aligns the sample structure with the national voter distribution, reducing bias and increasing the accuracy of our findings. Multi-layered data validation measures enhance the reliability and representativeness of the results.

**Poll Aggregation: Multi-Wave Polling and Adjustments**

To track changes in voter sentiment over time, we will conduct multiple waves of data collection and aggregation. The survey will be administered in multiple rounds throughout the election cycle, with each round spaced 3-4 weeks apart. Each wave of data will be collected and analyzed independently, then aggregated using weighted averages to smooth out single-instance fluctuations and help identify long-term trends, providing a reliable basis for predictions.

**Budget Allocation: Phased Spending and Testing**

To maximize budget efficiency, we will use a phased spending strategy. An initial 30% of the advertising budget will be allocated to testing across platforms to determine effectiveness, with the remaining 70% directed to the most successful channels. The budget breakdown is as follows: social media advertising ($40,000) for broad and targeted outreach, phone outreach ($20,000) for RDD calls to less accessible populations, small rewards ($15,000) to encourage completion, and data cleaning and analysis ($25,000) for validation, weighting, and aggregation across multiple waves.

**Survey Content and Link**

Survey Title: 2024 United States Presidential Election Survey

Survey Link: https://forms.gle/EzyHp3zuX8Cu6Ep8A

Introduction:

Thank you for taking part in our survey on the 2024 United States Presidential Election. Your input will help us understand voters’ views on candidates and key policy issues. All responses are confidential and will be used solely for statistical purposes. As a token of appreciation, participants will have a chance to win a small reward, with one in every 100 respondents receiving $5 to $10.

For Questions or Concerns, Please Contact:

Tianrui Fu

Email: tianrui.fu@mail.utoronto.ca

Yiyue Deng

Email: yiyue.deng@mail.utoronto.ca

Jianing Li

Email: lijianing.li@mail.utoronto.ca

Survey Questions:

1.	What is your age?

o	18-29

o	30-44

o	45-64

o	65 and above

2.	What is your gender?

o	Male

o	Female

o	Other

o	Prefer not to say

3.	What is your highest level of education?

o	High school or below

o	Bachelor’s degree

o	Master’s degree or above

4.	Which state do you currently reside in?

(Dropdown menu with state names)

5.	Do you plan to vote in the 2024 election?

o	Yes

o	No

o	Not sure

6.	If the election were held today, which candidate would you be more likely to support?

o	Kamala Harris (Democratic Party)

o	Donald Trump (Republican Party)

o	Other

o	Undecided

7.	Which of the following issues would have the greatest impact on your decision in the 2024 election? (Select one)

o	Economic stability and growth

o	Access to quality healthcare

o	Education reform and funding

o	Environmental sustainability and climate action

o	Addressing social and racial inequalities

o	Other (please specify): ________

8.	If a future candidate proposed a policy that aligns perfectly with your concerns, would you consider changing your voting intention?

o	Yes

o	No

o	Not sure

9.	If you would like to participate in the reward draw, please provide your email address below. Your contact information will only be used to notify winners. We will reach out to winners via email.

Email: ______________________

Thank You for Participating

We appreciate your time and input in our survey on the 2024 U.S. Presidential Election. Your responses are valuable to our research. Good luck with the reward draw, and have a great day!

## Clean data {#sec-cleandata}

It starts by reading the dataset and standardizing column names. Next, it removes irrelevant columns like sponsor_ids, sponsors, and etc and filters for high-quality polls (with a rating of 2.7 or higher) that specifically track Trump’s support. National polls missing state information are labeled as "National," and dates are formatted and filtered to include only those on or after July 21, 2024 (when Trump declared his candidacy). Finally, the percentage supporting Trump is converted to an actual count for further modeling. The same step also does with the Harris data set which will help us to do comparison later.

## Results of model {#sec-model_result}

### Table for summary of model results {#sec-tbl-model-summary}

In @tbl-modelresults, we conducted a summary of three models, including a generalized linear regression and a Bayesian model for Trump’s data, as well as a Bayesian model for Harris’s data. This summary includes the $\gamma_j$ values for each pollster and monitoring data for the models, such as AIC, BIC, etc.

```{r, fig.pos= 'H'}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: Explanatory models of flight time based on wing width and wing length
#| warning: false
#| message: false
library(modelsummary)
library(kableExtra)
library(dplyr)
coef_names <- c("end_date" = "End Date",
                "pollsterAtlasIntel" = "AtlasIntel",
                "pollsterBeacon/Shaw" = "Beacon/Shaw",
                "pollsterChristopher Newport U." = "Christopher Newport U.",
                "pollsterCNN/SSRS" = "CNN/SSRS",
                "pollsterData Orbital" = "Data Orbital",
                "pollsterEchelon Insights" = "Echelon Insights",
                "pollsterEmerson" = "Emerson",
                "pollsterIpsos" = "Ipsos",
                "pollsterMarist" = "Marist",
                "pollsterMarquette Law School" = "Marquette Law School",
                "pollsterMassINC Polling Group" = "MassINC Polling Group",
                "pollsterMcCourtney Institute/YouGov" = "McCourtney Institute/YouGov",
                "pollsterMuhlenberg" = "Muhlenberg",
                "pollsterQuinnipiac" = "Quinnipiac",
                "pollsterSelzer" = "Selzer",
                "pollsterSiena" = "Siena",
                "pollsterSiena/NYT" = "Siena/NYT",
                "pollsterSuffolk" = "Suffolk",
                "pollsterSurveyUSA" = "SurveyUSA",
                "pollsterSurveyUSA/High Point University" = "SurveyUSA/High Point University",
                "pollsterThe Washington Post" = "The Washington Post",
                "pollsterU. North Florida" = "U. North Florida",
                "pollsterUniversity of Massachusetts Lowell/YouGov" = "University of Massachusetts Lowell/YouGov",
                "pollsterWashington Post/George Mason University" = "Washington Post/George Mason University",
                "pollsterYouGov" = "YouGov",
                "pollsterYouGov Blue" = "YouGov Blue",
                "pollsterYouGov/Center for Working Class Politics" = "YouGov/Center for Working Class Politics")
                
modelsummary(models = list("Model by glm" = model_final_glm, 
                           "Model by bayes with Trump" = model_bayes_trump,
                           "Model by bayes with Harris" = model_bayes_harris),
             coef_rename = coef_names,
             output = "kableExtra") %>%
  kable_styling(full_width = FALSE, 
                position = "center", 
                latex_options = c("hold_position")) %>%
  column_spec(column = 1, width = "14em") %>%
  column_spec(column = 2:4, width = "14em") %>%
  add_header_above(c(" " = 1, "Model Summary" = 3)) %>%
  kable_classic(font_size = 5) %>%
  add_footnote("This table shows the regression models with custom variable names.")

```
\newpage
### Bayesian model for Harris data set {#sec-bayes-harris}

In @fig-bayes-harris, we applied a bayesian model to predict the percentage of polls Harris according each pollster. Each data point is color-coded by pollster, with support rates ranging from 35% to 65%. The shaded gray area around the trend line represents the confidence interval, indicating the uncertainty in support rate variations. The overall trend line (blue) shows a slight upward trend, increasing from about 46% in August to nearly 48% in October. Compared to Trump's model, the increasing trend of Harris is lower than Trump's. By October, most poll results have converged, with support rates centered between 45% and 50%.

```{r, fig.pos= 'H', fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: fig-bayes-harris
#| fig-cap: Poll Percentage over Time with Bayesian Fit for Harris
#| warning: false

new_data <- data.frame(
  end_date = seq(
    min(analysis_harris$end_date),
    max(analysis_harris$end_date),
    length.out = 100
  ),
  numeric_grade = mean(analysis_harris$numeric_grade),
  pollscore = mean(analysis_harris$pollscore),
  transparency_score = mean(analysis_harris$transparency_score),
  pollster = factor("AtlasIntel")
)

posterior_preds <- posterior_predict(model_bayes_harris, newdata = new_data)


pred_summary <- new_data |>
  mutate(
    pred_mean = colMeans(posterior_preds),
    pred_lower = apply(posterior_preds, 2, quantile, probs = 0.025),
    pred_upper = apply(posterior_preds, 2, quantile, probs = 0.975)
  )


ggplot(analysis_harris, aes(x = end_date, y = pct, color = pollster)) +
  geom_point() +
  geom_line(
    data = pred_summary,
    aes(x = end_date, y = pred_mean),
    color = "blue",
    inherit.aes = FALSE
  ) +
  geom_ribbon(
    data = pred_summary,
    aes(x = end_date, ymin = pred_lower, ymax = pred_upper),
    alpha = 0.2,
    inherit.aes = FALSE
  ) +
  labs(
    x = "End Date",
    y = "Percentage"
  ) +
  theme_minimal() +
  theme(
    legend.key.size = unit(1, "cm"),
    legend.text = element_text(size = 5) 
  )

```
\newpage
### Predictions for both Trump and Harris {#sec-pred-summary}

In @tbl-all-pred, we summarized the Bayesian model predictions for Trump and Harris on November 5, 2024, based on 29 pollsters, including the mean, lower bound, and upper bound.

```{r, fig.pos= 'H'}
#| echo: false
#| eval: true
#| label: tbl-all-pred
#| tbl-cap: predictions for trump and harris by pollster
#| warning: false

all_pollsters_trump <- unique(analysis_trump$pollster)
all_pollsters_harris <- unique(analysis_harris$pollster)

all_predictions <- data.frame()

for (pollster in all_pollsters_trump) {
  new_data_trump <- data.frame(
    end_date = as.Date("2024-11-05"),
    numeric_grade = mean(analysis_trump$numeric_grade, na.rm = TRUE),
    pollscore = mean(analysis_trump$pollscore, na.rm = TRUE),
    transparency_score = mean(analysis_trump$transparency_score, na.rm = TRUE),
    pollster = factor(pollster) 
  )
  
  posterior_preds_trump <- posterior_predict(model_bayes_trump, newdata = new_data_trump)
  
  final_prediction_trump <- data.frame(
    pollster = pollster, 
    candidate = "Trump",
    pred_mean = mean(posterior_preds_trump),
    pred_lower = quantile(posterior_preds_trump, probs = 0.025),
    pred_upper = quantile(posterior_preds_trump, probs = 0.975)
  )
  
  all_predictions <- rbind(all_predictions, final_prediction_trump)
}

for (pollster in all_pollsters_harris) {
  new_data_harris <- data.frame(
    end_date = as.Date("2024-11-05"),
    numeric_grade = mean(analysis_harris$numeric_grade, na.rm = TRUE),
    pollscore = mean(analysis_harris$pollscore, na.rm = TRUE),
    transparency_score = mean(analysis_harris$transparency_score, na.rm = TRUE),
    pollster = factor(pollster) 
  )
  
  posterior_preds_harris <- posterior_predict(model_bayes_harris, newdata = new_data_harris)
  
  final_prediction_harris <- data.frame(
    pollster = pollster, 
    candidate = "Harris",
    pred_mean = mean(posterior_preds_harris),
    pred_lower = quantile(posterior_preds_harris, probs = 0.025),
    pred_upper = quantile(posterior_preds_harris, probs = 0.975)
  )
  
  all_predictions <- rbind(all_predictions, final_prediction_harris)
}

kable(all_predictions, col.names = c("Pollster", "Candidate", "Predicted Mean", "Lower Bound", "Upper Bound"), booktabs = TRUE) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position")) %>%
  column_spec(1, width = "2cm") %>%
  column_spec(2, width = "3cm") %>%
  column_spec(3:5, width = "2cm") %>%
  add_header_above(c(" " = 1, "Predictions Summary" = 5)) %>%
  row_spec(0, bold = TRUE)


```

# Model details {#sec-model-details}

To maintain readability while demonstrating model robustness, we include the following in the appendix:

- **Prior Justification and Sensitivity Analyses:** Alternative priors and their justifications are provided, alongside a sensitivity analysis to examine the impact of these priors on the posterior distributions.

- **Model Validation and Out-of-Sample Testing:** Validation metrics, such as RMSE calculations, out-of-sample testing, and test-train splits, offer evidence of the model’s predictive accuracy, complementing in-sample performance.

- **Alternative Models and Comparison:** An analysis of simpler and more complex models is included, explaining the rationale for selecting this Bayesian model based on performance metrics and interpretability.

\newpage


# References


